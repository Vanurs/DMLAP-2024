{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Classification\n",
    "\n",
    "In this notebook we are going to train an audio classifier on the [Speech Commands dataset](https://drive.usercontent.google.com/download?id=1J0rGy64nRSNdAjgfDXzPHs_YHkvAiP8-). It is a multi-class classification problem. After running the notebook as it is, you are invited to use the same notebook to train a model on your custom dataset. \n",
    "\n",
    "The network of this notebook is a 1-Dimensional CNN, similar to the networks we saw in week 5 for image classification. The specific architecture is modeled after the M5 network architecture described in [this paper](https://arxiv.org/pdf/1610.00087.pdf). You can read more about Speech Command Classification with torchaudio in [PyTorch Tutorial page](https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html) which is also the reference point for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from src.audio_dataloader import AudioFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the datasets folder exists\n",
    "# if not, create it\n",
    "PATH = './datasets'\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can skip this step if you create your dataset folder manually\n",
    "# with your custom classes\n",
    "dataset = torchaudio.datasets.SPEECHCOMMANDS('./datasets' , url = 'speech_commands_v0.02', folder_in_archive= 'SpeechCommands',  download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the dataset to the datasets folder manually or:\n",
    "\n",
    "# *** for MAC and Linux OS ***\n",
    "# !mv SpeechCommands ./datasets # move the dataset in the designated directory\n",
    "\n",
    "# *** for Windows OS ***\n",
    "# !robocopy SpeechCommands ./datasets/ /E # copy and paste unzipped dataset to the designated directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_rate = 8000\n",
    "batch_size = 256\n",
    "learn_rate = 1e-2\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 2\n",
    "val_size = 0.3\n",
    "# root directory for dataset\n",
    "dataroot = './datasets/SpeechCommands/speech_commands_v0.02'\n",
    "# path to new model\n",
    "save_path = 'models/model.pt'\n",
    "# replace mps with cpu if not using M1/M2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' \n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metadata of a sample audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a sample audio file from the training dataset and look into its metadata\n",
    "sample_wav = dataroot + '/bed/0a7c2a8d_nohash_0.wav'\n",
    "metadata = torchaudio.info(sample_wav)\n",
    "waveform, sample_rate = torchaudio.load(sample_wav)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listen to audio with its original sample rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(waveform.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listen to audio after transformations are applied\n",
    "\n",
    "Compare the sound of the file before and after downsampling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n",
    "\n",
    "transformed = transform(waveform)\n",
    "ipd.Audio(transformed.numpy(), rate=new_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our dataset\n",
    "\n",
    "##### To be splitted into train and validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sneak peak into the dataset classes\n",
    "dataset = AudioFolder(dataroot, transform=transform)\n",
    "print(len(dataset), len(dataset.classes), dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code cell from PyTorch Tutorial\n",
    "# https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html\n",
    "# to make all tensors in a batch of the same length\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, label in batch:\n",
    "        tensors += [torch.Tensor(waveform)]\n",
    "        targets += [label]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = np.stack(targets)\n",
    "    targets = torch.Tensor(targets)\n",
    "    targets = targets.to(torch.long)\n",
    "\n",
    "    return tensors, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of idx numbers for each element of the full dataset\n",
    "dataset_size = len(dataset)\n",
    "idx = list(range(dataset_size))\n",
    "\n",
    "# perform train / val split for data points\n",
    "# by setting `random_state=42`, we are doing the split in a deterministic way, \n",
    "# ie. we will always get the same 'random' split of data into the training and validation subsets\n",
    "train_indices, val_indices = train_test_split(idx, test_size=val_size, random_state=42)\n",
    "\n",
    "# override datasets to only be samples for each split\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch, label_batch = next(iter(train_loader))\n",
    "sample_waveform = data_batch[0].squeeze()\n",
    "print(f'Data batch shape: {data_batch.shape}')\n",
    "print(f\"Shape of waveform: {sample_waveform.size()}\")\n",
    "sample_class = int(label_batch[0].item())\n",
    "print(f'Class of waveform: \\'{dataset.classes[sample_class]}\\'')\n",
    "plt.plot(sample_waveform.t().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model architecture\n",
    "\n",
    "1-Dimensional Convolutional Neural Network to process raw audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M5(n_input=1, n_output=len(dataset.classes))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# the number of parameters to be learnt\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)\n",
    "\n",
    "# optimiser\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval, losses):\n",
    "    # put model is training mode\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # move data into the designated device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward pass through the model\n",
    "        output = model(data)\n",
    "\n",
    "        # evaluate based on the difference between predicted output and original target\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        # backpropagate loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # to be used for cumulative loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # update the graphic bar that shows the progress of training per batch\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    \n",
    "    # get cumulative loss for each batch\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    losses.append(train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation functions\n",
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "# validation function\n",
    "def val(model, epoch, losses):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for data, target in val_loader:\n",
    "            \n",
    "            # move data into the designated device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # forward pass through the model\n",
    "            output = model(data)\n",
    "\n",
    "            # evaluate based on the difference between predicted output and original target\n",
    "            loss = criterion(output.squeeze(), target)\n",
    "\n",
    "            # to be used for cumulative loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # use evaluation functions defined above\n",
    "            pred = get_likely_index(output)\n",
    "            correct += number_of_correct(pred, target)\n",
    "            \n",
    "            # update the graphic bar that shows the progress of training per batch\n",
    "            pbar.update(pbar_update)\n",
    "\n",
    "        val_loss = val_loss / len(train_loader)\n",
    "        losses.append(val_loss)\n",
    "\n",
    "    print(f\"\\nValidation Epoch: {epoch}\\tAccuracy: {correct}/{len(val_loader.dataset)} ({100. * correct / len(val_loader.dataset):.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 20\n",
    "pbar_update = 1 / (len(train_loader) + len(val_loader))\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_loss = 10000\n",
    "\n",
    "# while tracking the graphic bar of progress\n",
    "with tqdm(total=num_epochs) as pbar:\n",
    "    # call the train and val functions for each epoch of training\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train(model, epoch, log_interval, train_losses)\n",
    "        val(model, epoch, val_losses)\n",
    "        # save the model if the last val_loss added is better than the previous one\n",
    "        if val_losses[-1] < best_loss:\n",
    "            best_loss = val_losses[-1]\n",
    "            torch.save(model.cpu().state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Train n' Validation Loss\")\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cumulative loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference\n",
    "\n",
    "Test the performance of the model with unseen data from the test subfolder of speech commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "state_dict = torch.load('./models/model.pt') # add the path to your saved model, if different\n",
    "\n",
    "# instantiate the model and put it in evaluation mode\n",
    "model = M5(n_input=1, n_output=len(dataset.classes))\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio file from your custom path\n",
    "test_wav = './PATH/TO/AUDIO-SAMPLE.wav'\n",
    "\n",
    "# check if the file exists\n",
    "if not os.path.exists(test_wav):\n",
    "    print(\"Error: Audio file does not exist.\")\n",
    "else:\n",
    "    # load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(test_wav)\n",
    "    # check if the duration of the audio is too short\n",
    "    min_duration = 1.0  # minimum duration in seconds\n",
    "    # print(waveform.size(1))\n",
    "    if waveform.size(1) / sample_rate < min_duration:\n",
    "        print(\"Error: Audio duration is too short.\")\n",
    "    else:\n",
    "        # proceed with inference\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n",
    "        transformed = transform(waveform)\n",
    "        # print(transformed.shape)\n",
    "        input_data = transformed.unsqueeze(0)  # add batch dimension\n",
    "        # print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_data)\n",
    "\n",
    "# get the predicted label\n",
    "pred = output.argmax(dim=-1)\n",
    "pred_label = dataset.classes[pred.item()]\n",
    "\n",
    "print(\"Predicted label:\", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it what the model predicted?\n",
    "ipd.Audio(transformed.numpy(), rate=new_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
