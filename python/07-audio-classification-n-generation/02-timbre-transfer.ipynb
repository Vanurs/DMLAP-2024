{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timbre Transfer for Audio Generation\n",
    "\n",
    "In this notebook we are going to use [RAVE](https://github.com/acids-ircam/RAVE) - a Realtime Audio Variational Autoencoder to generate sound. You can read the respective paper [here](https://arxiv.org/abs/2111.05011). \n",
    "\n",
    "RAVE is a very light model that allows for generating audio in real-time in the CPU. However, note that we will be using pre-trained models, navigating in their latent space and generating audio from them. More specifically, we will be performing timbre transfer, which is the process of passing in the encoder an existing audio file and expecting the decoder export an audio file with an altered timbre. This notebook is not about training a new RAVE model, which is very a computationally demanding process.\n",
    "\n",
    "This notebook is adapted from Iran R. Roman (iran [@] ccrma.stanford.edu) and [Teresa Pelinski](https://teresapelinski.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install librosa if you get an error in imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as li\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load a sample audio file\n",
    "\n",
    "You will find a few sound sound files in the `sounds` folder. Feel free to populate it with more. You can look into [freesound.org](https://freesound.org). In the code cell below, we download an audio sample instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a sample audio file\n",
    "def download_file(url, file_path):\n",
    "    \"\"\"\n",
    "    Download file from a given URL and save it to the specified file path\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # this will raise an exception if there is an error\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "url = 'https://ccrma.stanford.edu/~jos/wav/gtr-nylon22.wav'\n",
    "audio_path = './sounds/audio-sample.wav'\n",
    "download_file(url, audio_path)\n",
    "\n",
    "input_data, sample_rate = li.load(audio_path)\n",
    "display(Audio(input_data, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualise the audiowave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0, len(input_data)) / sample_rate # to obtain the time in seconds, we need to divide the sample index by the sample rate\n",
    "plt.plot(time, input_data)\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(audio_path.split(\"/\")[-1])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download one or more pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download rave parameters/weights and build the model\n",
    "url = 'https://play.forum.ircam.fr/rave-vst-api/get_model/percussion'\n",
    "# url = 'https://play.forum.ircam.fr/rave-vst-api/get_model/vintage'\n",
    "# url = 'https://play.forum.ircam.fr/rave-vst-api/get_model/nasa'\n",
    "# url = 'https://play.forum.ircam.fr/rave-vst-api/get_model/darbouka_onnx'\n",
    "# url = 'https://play.forum.ircam.fr/rave-vst-api/get_model/VCTK'\n",
    "# you can learn more about each model at:\n",
    "# https://acids-ircam.github.io/rave_models_download\n",
    "\n",
    "#model_path = './models/percussion.ts'\n",
    "# Automatically fetches model name from url:\n",
    "model_path = os.path.join('./models', os.path.basename(url) + '.ts')\n",
    "\n",
    "download_file(url, model_path)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "model = torch.jit.load(model_path).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.join('./models', os.path.basename(url) + '.ts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform timbre transfer on the sample audio file\n",
    "\n",
    "We can now load a pretrained model using `torch.jit.load` and encode the input audio into a latent representation. For the percussion model, we will be encoding our input audio into a latent space trained on 8h of various percussion recordings. We can then decode the latent representation and synthesise it. This will make the original sound as if it was percussion (timbre transfer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, sample_rate = li.load(audio_path)\n",
    "input_data = torch.from_numpy(input_data).reshape(1, 1, -1)\n",
    "\n",
    "# encode and decode the audio with RAVE\n",
    "# to synthesize audio from latent representation\n",
    "z = model.encode(input_data) # encode the signal into the latent representation\n",
    "y = model.decode(z).numpy() # decode latent representation and convert tensor to numpy array\n",
    "y = y[:,0,:].reshape(-1) # remove batch and channel dimensions\n",
    "\n",
    "# sf.write(\"model_output.wav\", y, sample_rate) # to save the file\n",
    "display(Audio(y, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform timbre transfer on randomness\n",
    "\n",
    "Sample random points in the latent space and generate sound clips out of them. Then, concantenate them all into a continuous clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 8\n",
    "\n",
    "generated_clips = []\n",
    "for i in range(100):\n",
    "    # randomly sample latent space\n",
    "    z = torch.randn(1,latent_dim,1)\n",
    "    \n",
    "    # generate audio clip and append to list\n",
    "    gen_audio_clip = model.decode(z)\n",
    "    gen_audio_clip = gen_audio_clip.reshape(-1).cpu().numpy()\n",
    "    generated_clips.append(gen_audio_clip)\n",
    "\n",
    "# concantenate list of audio clips into one array\n",
    "y = np.concatenate(generated_clips)\n",
    "# sf.write(\"random_output.wav\", y, sample_rate) # to save the file\n",
    "display(Audio(y, rate=sample_rate)) # display audio widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Altering latent representations\n",
    "\n",
    "Modify the latent coordinates of the input file to alter the representation. We can start by adding a constant bias (a displacement) to the coordinates in the latent space. Note that each RAVE model has a different number of coordinates for its latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions of the latent space\n",
    "# the second dimension corresponds to the latent dimension\n",
    "print(z.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, sample_rate = li.load(audio_path) #'audio.wav')\n",
    "x = torch.from_numpy(input_data).reshape(1, 1, -1)\n",
    "\n",
    "z = model.encode(x) # encode the signal into the latent representation\n",
    "\n",
    "z_modified = torch.clone(z) # copy latent representation\n",
    "\n",
    "d0 = 2.0  # change that will occur in latent dimension 0\n",
    "d1 = 1.2 \n",
    "d2 = 0.05\n",
    "d3 = 0.8\n",
    "# you can add and play with more of the available dimensions\n",
    "\n",
    "z_modified[:, 0] += torch.linspace(-d0, d0, z.shape[-1])\n",
    "z_modified[:, 1] += torch.linspace(-d1,d1, z.shape[-1])\n",
    "z_modified[:, 2] += torch.linspace(-d2,d2, z.shape[-1])\n",
    "z_modified[:, 3] += torch.linspace(-d3,d3, z.shape[-1])\n",
    "\n",
    "y = model.decode(z_modified).numpy().reshape(-1)\n",
    "# sf.write(\"latent_output.wav\", y, sample_rate) # save output audio\n",
    "display(Audio(y, rate=sample_rate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now move to MAX\n",
    "\n",
    "Move into the MAX patch `rave-in-max` and find the analogies between what we just did in the last cell and what is happening there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
