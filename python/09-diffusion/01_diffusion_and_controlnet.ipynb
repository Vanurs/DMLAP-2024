{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable diffusion with Diffusers\n",
    "\n",
    "This is a notebook that demonstrates how to use the [Diffusers package](https://huggingface.co/docs/diffusers/index) from Huggingface to run stable diffusion. [Huggingface](https://huggingface.co) is a community-driven  platform that provides a comprehensive suite of open-source libraries and tools for machine learning. Think of it as a kind of \"github for machine learning\". The diffusers package provides a simple API and a number of pretrained models that allow to easily run and experiment with diffusion models. \n",
    "\n",
    "## Installation\n",
    "To use this notebook you will need to install diffusers (with your conda environment active) with:\n",
    "```\n",
    "pip install --upgrade diffusers\\[torch\\]\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  ONLY IF USING COLAB, run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade diffusers\\[torch\\]\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then mount your Google Drive so you have access to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "import os\n",
    "os.chdir('drive/My Drive/DMLAP-2024/python') # change to a directory of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make sure that you have the \"images\" directory from this week's \"09-diffusion\" copied inside the folder of choice. Alternatively, you may manually upload the dictory to Collab, but it will be gone once you close the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  END COLAB NOTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running stable diffusion\n",
    "To run stable diffusion you will need to distinguish wether you are running this on a Mac M1/M2 or on Linux/Windows with a Nvidia GPU. \n",
    "On a M1/M2 Mac, diffusion will need a \"warmup\" phase to work properly (see [this link](https://huggingface.co/docs/diffusers/optimization/mps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this will load the pretrained model and download it the first time the cell is run (which might take a while). You can set the model version by modifying the `sd_version` variable. Read [this document](https://huggingface.co/docs/diffusers/en/stable_diffusion) for performance recommendations and tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "sd_version = '2.1'\n",
    "\n",
    "if sd_version == '2.1':\n",
    "    model_key = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "elif sd_version == '2.0':\n",
    "    model_key = \"stabilityai/stable-diffusion-2-base\"\n",
    "elif sd_version == '1.5':\n",
    "    model_key = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_key, torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) # Faster scheduler\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    # Recommended if your computer has < 64 GB of RAM\n",
    "    pipe.enable_attention_slicing()\n",
    "if device=='cuda':\n",
    "    pipe.enable_sequential_cpu_offload()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the image (note increasing `num_inference_steps` will improve quality but be slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.manual_seed(99) # CPU generator \n",
    "\n",
    "prompt = \"A cubist painting of the Star Treck character Spock, high quality, trending on artstation\"\n",
    "image = pipe(prompt, guidance_scale=7.5, generator=generator, num_inference_steps=20).images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text guided image to image generation\n",
    "We can use stable diffusion also with a \"seed image\", so let's load Spock. Note that using the `PIL.Image` class, we can view an image by simply putting the variable as the last statement of a cell.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "\n",
    "seed_image = Image.open(\"images/spock.jpg\")\n",
    "seed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the [`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/en/api/pipelines/stable_diffusion/img2img) model, similarly to what we did earlier with the normal stable diffusion pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_key, torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) # Faster scheduler\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    # Recommended if your computer has < 64 GB of RAM\n",
    "    pipe.enable_attention_slicing()\n",
    "if device=='cuda':\n",
    "    pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StableDiffusionImg2ImgPipeline` takes an additional `image` parameter that conditions the diffusion process so that the result is similar to a given image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A punk with a mohawk\"\n",
    "\n",
    "generator = torch.manual_seed(32) #torch.Generator(device).manual_seed(0) # Removing manual_seed will always generate different images \n",
    "image = pipe(prompt, image=seed_image, guidance_scale=7.5, strength=0.7, generator=generator, num_inference_steps=20).images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well at least we can try...\n",
    "\n",
    "The diffusers API provides a number of [other pipelines](https://huggingface.co/docs/diffusers/en/api/pipelines/stable_diffusion/overview) that can be useful for other tasks, such as image inpainting or depth-to-image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning stable diffusion with ControlNet\n",
    "\n",
    "[ControlNet](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) is a very recent and quite amazing advancement in image generation using stable diffusion. It allows conditioning the stable diffusion generation pipeline on an image input (similarly to pix2pix).  This system operates by integrating a smaller neural network with a pre-trained stable diffusion model. The weights of the stable diffusion model are frozen, and the combined model is trained to steer stable diffusion towards producing images consistent with the provided input conditions.\n",
    "\n",
    "The Huggingface diffusers API comes with ControlNet and a number of pre-trained models that can be used for an number of tasks such as guiding stable diffusion with edges, poses or depth maps (see [https://huggingface.co/lllyasviel/sd-controlnet-canny](https://huggingface.co/lllyasviel/sd-controlnet-canny) for some available models).\n",
    "\n",
    "Here we will use the [\"sd-controlnet-canny\"](https://huggingface.co/lllyasviel/sd-controlnet-canny) model, which guides stable diffusion with edge images. Let's start by using skimage to create edges from an input image, as we did with pix2pix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io, feature, transform\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "\n",
    "def apply_canny_skimage(img, sigma=1.5, size=512):\n",
    "    import cv2\n",
    "    from skimage import feature\n",
    "    invert = False\n",
    "    grayimg = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = (feature.canny(grayimg, sigma=sigma)*255).astype(np.uint8)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "img = io.imread(\"images/spock.jpg\")\n",
    "edges =  apply_canny_skimage(img)\n",
    "\n",
    "# ControlNet expects a PIL Image as input\n",
    "edges_image = Image.fromarray(edges)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(edges)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the ControlNet model. Note that the current HuggingFace version of ControlNet requires Stable Diffusion 1.5 to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import PIL.Image as Image\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\", \n",
    "    torch_dtype=torch.float16)\n",
    "    \n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", # Controlnet is currently working only with SD 1.5\n",
    "    torch_dtype=torch.float16,\n",
    "    controlnet=controlnet, \n",
    "    safety_checker=None)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) # Faster scheduler\n",
    "print(\"converting to device\")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    pipe.enable_attention_slicing()\n",
    "if device=='cuda':\n",
    "    pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate an image, by passing the edge image as an input to the `image` argument. We will use 15 inference steps here to save a little bit of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A sculpture made of clay on a white background\"\n",
    "\n",
    "generator = torch.manual_seed(0) # Removing manual_seed will always generate different images \n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=edges_image,\n",
    "    generator=generator,\n",
    "    num_inference_steps=15,\n",
    ").images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedural input\n",
    "\n",
    "In the previous example, we used edge detection to produce the ControlNet condition image. However, you can create interesting visual results also by procedurally generating an \"edge-like\" image, similarly to some examples we used with Pix2Pix. We will use the [py5canvas](https://github.com/colormotor/py5canvas) API here to make things easier (as long as you are able to install it correctly). Here are some updated instructions that should work, in case you have not installed it yet. **Skip these steps if you have already installed and used py5canvas**. \n",
    "\n",
    "With you environment active:\n",
    "```\n",
    "pip install face_recognition\n",
    "pip install pyglet\n",
    "```\n",
    "Then:\n",
    "```\n",
    "conda install conda-forge::cairo\n",
    "```\n",
    "Followed by:\n",
    "```\n",
    "conda install conda-forge::pycairo\n",
    "```\n",
    "Finally install py5canvas with: \n",
    "```\n",
    "pip install git+https://github.com/colormotor/py5canvas.git\n",
    "```\n",
    "\n",
    "Now we can crete some graphics:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  ONLY IF USING COLAB, run this first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install libcairo2-dev pkg-config python3-dev\n",
    "!pip install git+https://github.com/colormotor/py5canvas.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  END COLAB NOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py5canvas import canvas\n",
    "c = canvas.Canvas(512, 512)\n",
    "\n",
    "np.random.seed(123)\n",
    "c.background(0)\n",
    "c.stroke_weight(2)\n",
    "c.stroke(255)\n",
    "c.no_fill()\n",
    "#c.set_rect_mode('center')\n",
    "for i in range(40):\n",
    "    c.rectangle(np.random.uniform(0, c.width, 2), np.random.uniform(10, 90, 2))\n",
    "\n",
    "# Retrieve the canvas image and convert it from a numpy array to a PIL Image. \n",
    "img = c.get_image()\n",
    "edges_image = Image.fromarray(img)\n",
    "# show it\n",
    "edges_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we may get better results by using the [\"sd-controlnet-scribble\"](https://huggingface.co/lllyasviel/sd-controlnet-scribble) model, which is similar to the Canny edge detection model, but is trained on scribble-like images instead of edge maps. Note that both the \"scribble\" and \"canny\" control net are trained on images with that have a specific line width and the stroke width of the generated condition image will affect the quality of the results. Try to adjust the line width using the `stroke_weight` function and tweak it to adjust the generated results to your preference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-scribble\", \n",
    "    torch_dtype=torch.float16)\n",
    "    \n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", # Controlnet is currently working only with SD 1.5\n",
    "    torch_dtype=torch.float16,\n",
    "    controlnet=controlnet, \n",
    "    safety_checker=None)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) # Faster scheduler\n",
    "print(\"converting to device\")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "if device=='mps':\n",
    "    pipe.enable_attention_slicing()\n",
    "if device=='cuda':\n",
    "    pipe.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will also add a \"negative prompt\", which is a caption describing outputs we wish to avoid. This time we reduce the inference steps to 10, to make the process a bit faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A white wall with many portraits of Star Trek character Spock\"\n",
    "\n",
    "generator = torch.manual_seed(420) # Removing the generator will always result in different images \n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=\"blurry, user interface, captions\",\n",
    "    image=edges_image,\n",
    "    generator=generator,\n",
    "    controlnet_conditioning_scale=0.9,\n",
    "    guidance_scale=7.5, \n",
    "    guess_mode=False, # Guess mode will try to \"guess\" the contents of the input, so it can be used also without a prompt\n",
    "    num_inference_steps=10,\n",
    ").images[0]\n",
    "\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(prompt)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedural input with animations\n",
    "We can also create a procedural animation using a similar technique together with ffmpeg. Let's create an animation of waves, saving each frame of the animation in a \"input_animation_frames\" directory (for visualization) and storing the frames in a `frame_images` list. For time/performance consideration, we will create only 10 frames for the moment. This will result in a bit of a choppy animation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from py5canvas import canvas\n",
    "\n",
    "# Create a folder for input animation \n",
    "output_folder = 'input_animation_frames'\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "# Modify this function for different animations\n",
    "def animation_frame(c, frame_index, n_frames):\n",
    "    c.background(0)\n",
    "    c.stroke_weight(1) \n",
    "    c.stroke(255)\n",
    "    c.no_fill()\n",
    "\n",
    "    phase = (np.pi*2)/n_frames * frame_index\n",
    "    for y in np.linspace(0, 1, 13)[1:-1]: # Loop 0-1 skipping first and last row (\n",
    "        c.begin_shape()\n",
    "        for x in np.linspace(0, 1, 100):\n",
    "            c.vertex(x*c.width, y*c.height +\n",
    "                     np.sin(x*np.pi*4 + phase + y*6)*np.cos(y*np.pi*6 + x*5.2)*20) # Bit arbitrary moving waves creating a loop\n",
    "        c.end_shape()\n",
    "    c.save_image(os.path.join(output_folder, 'frame_%02d.png'%(frame_index+1)))\n",
    "    return Image.fromarray(c.get_image())\n",
    "\n",
    "# Create the canvas once\n",
    "c = canvas.Canvas(512, 512)\n",
    "\n",
    "# comment this block and uncomment the last line to preview one frame\n",
    "n_frames = 10\n",
    "frame_images = []\n",
    "for i in range(n_frames):\n",
    "    frame_images.append( animation_frame(c, i, n_frames) )\n",
    "# preview one frame\n",
    "frame_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above previews one frame of the animtion. Now let's use ffmpeg to convert the saved frames to an animated gif that we can visualize in the notebook. We will use a very low frame rate (10) so the animation with 10 frames will last one second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -f image2 -framerate 10 -i 'input_animation_frames/frame_%02d.png' -loop 0 input_animation.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above (as other command-line commands can be) is quite cryptic to read. ChatGPT can be a useful tool here, e.g. try to write \"Translate this to English\" followed by the ffmpeg command.\n",
    "\n",
    "Now to visualize the animation we can use the `IPython` module with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Image('input_animation.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take the image for each frame and use ControlNet to to transform it. Note that we force the seed to be the same for each iteration of the loop. This will give some form of \n",
    "temporal coherence to the animation. Nevertheless, the animation will probably flicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for input animation \n",
    "output_folder = 'output_animation_frames'\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "prompt = \"Colorful 3d ribbons, Cinema 4d, Maxon renderer\"\n",
    "preview = -1 # Set to 0 or greater to just preview a frame, -1 for saving video\n",
    "generated_images = []\n",
    "for i, frame in enumerate(frame_images):\n",
    "    print('Frame %d of %d'%(i+1, len(frame_images)))\n",
    "    generator = torch.manual_seed(677) # Removing the generator will always result in different images \n",
    "    if preview > -1:\n",
    "        frame = frame_images[preview]\n",
    "\n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=\"blurry, user interface, captions\",\n",
    "        image=frame,\n",
    "        generator=generator,\n",
    "        controlnet_conditioning_scale=0.7,\n",
    "        guidance_scale=6.5, \n",
    "        guess_mode=False, # Guess mode will try to \"guess\" the contents of the input, so it can be used also without a prompt\n",
    "        num_inference_steps=10, # 10 inference steps is faster and seems to be enough\n",
    "    ).images[0]\n",
    "    if preview > -1:\n",
    "        display(image)\n",
    "        break\n",
    "    generated_images.append(image)\n",
    "    image.save(os.path.join(output_folder, 'frame_%02d.png'%(i+1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's create a gif, this time from the \"output_animation_frames\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -f image2 -framerate 10 -i 'output_animation_frames/frame_%02d.png' -loop 0 output_animation.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image('output_animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
