{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foXvSQKubUBS"
      },
      "source": [
        "# Homework - Chatbots with Gradio\n",
        "\n",
        "Gradio is a web app framework designed to facilitate the development and deployment of ML and DL apps. Have a look at [their website](https://www.gradio.app).\n",
        "\n",
        "The following adapts their [Quickstart Guide](https://www.gradio.app/guides/quickstart).\n",
        "\n",
        "Notebook by [JÃ©rÃ©mie C. Wegner](https://jeremiewenger.com/about/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IHmAEQ6YEzgj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (4.21.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (5.2.0)\n",
            "Requirement already satisfied: fastapi in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.110.0)\n",
            "Requirement already satisfied: ffmpy in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.12.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (3.8.2)\n",
            "Requirement already satisfied: numpy~=1.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (1.26.3)\n",
            "Requirement already satisfied: orjson~=3.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (3.9.15)\n",
            "Requirement already satisfied: packaging in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (10.2.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (2.6.3)\n",
            "Requirement already satisfied: pydub in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.9 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio) (0.28.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio-client==0.12.0->gradio) (2023.10.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from gradio-client==0.12.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from httpx>=0.24.1->gradio) (4.2.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.4)\n",
            "Requirement already satisfied: idna in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from fastapi->gradio) (0.36.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.15.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\46583\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k892p0HVh7cZ"
      },
      "source": [
        "### Example using a local, open-source LLM with Hugging Face\n",
        "\n",
        "See [this part](https://www.gradio.app/guides/creating-a-chatbot-fast#example-using-a-local-open-source-llm-with-hugging-face).\n",
        "\n",
        "The model we use is [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1) (pretty big, best run on Colab!), see also [this post for the dataset](https://www.together.ai/blog/redpajama-data-v2). Chat models are regular language models finetuned on specific chat datasets (especially, they include markers for \"user input\" and \"assistant responses\", as well as, sometimes, overall directives like \"system prompt\" (defining the overall identity of the bot). In Huggingface, you would recognise them as having a \"-chat\" identifier, for instance for the [Llama 2 family](https://huggingface.co/meta-llama).\n",
        "\n",
        "Docs:\n",
        "- [StopingCriteria](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.StoppingCriteria) \\(see also [this nice post](https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/2)\\)\n",
        "- [TextIteratorStreamer](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextIteratorStreamer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KRhQNjVTpSMC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Get cpu, gpu or mps device for training.\n",
        "# See: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import StoppingCriteria\n",
        "from transformers import StoppingCriteriaList\n",
        "from transformers import TextIteratorStreamer\n",
        "\n",
        "from threading import Thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTuRsLf7QEqG"
      },
      "source": [
        "#### Note\n",
        "\n",
        "This chatbot uses almost all the memory of a free Colab instance. Unfortunately, I haven't been able to free the memory so that I would be able to restart this app for debugging without restarting the runtime (and re-downloading the model) ðŸ˜¬.\n",
        "\n",
        "The upside is: it is quite powerful! Try speak to it in different languages, or ask it code questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d0577f6ee26453cbe30ed1175c0473d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from huggingface_hub import notebook_login\n",
        "if not (Path.home()/'.huggingface'/'token').exists():\n",
        "    notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4drDf7PRg2n_"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1886dbdc201b450fa08e396b81ac99a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ac5ab99833b4d60b0da0c522964a3ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e10a6491c5e14310b30145513dddd4a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a07126f95454700a7e52f48035a5687",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2ae0708651b4c3f89b9c2a8410aec22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0758fcb5d94b4c03afd3d0fc0dfea945",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adffd090aa1e42f9b15cb65891da1911",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbbdc556e4534494bc5de76e80ffc1d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7aab0eee1504c42979198ac9101385e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d89a202ccbb4659bcd00b3d88f1b540",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f0d581f22284e05a32c7a5490ea7d8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.28 GiB is allocated by PyTorch, and 1.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_ID)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_ID, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# move model to GPU\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStopOnTokens\u001b[39;00m(StoppingCriteria):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    Class used `stopping_criteria` in `generate_kwargs` that provides an additional\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    way of stopping the generation loop (if this class returns `True` on a token,\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    the generation is stopped)).\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages\\transformers\\modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2555\u001b[0m         )\n\u001b[1;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "    \u001b[1;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[1;32mc:\\Users\\46583\\anaconda3\\envs\\dmlap2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.28 GiB is allocated by PyTorch, and 1.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16)\n",
        "model = model.to(device) # move model to GPU\n",
        "\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    \"\"\"\n",
        "    Class used `stopping_criteria` in `generate_kwargs` that provides an additional\n",
        "    way of stopping the generation loop (if this class returns `True` on a token,\n",
        "    the generation is stopped)).\n",
        "    \"\"\"\n",
        "    # note: Python now supports type hints, see this: https://realpython.com/lessons/type-hinting/\n",
        "    #       (for the **kwargs see also: https://realpython.com/python-kwargs-and-args/)\n",
        "    # this could also be written: def __call__(self, input_ids, scores, **kwargs):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        stop_ids = [29, 0] # see the cell below to understand where these come from\n",
        "        for stop_id in stop_ids:\n",
        "            if input_ids[0][-1] == stop_id:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "def predict(message, history):\n",
        "\n",
        "    history_transformer_format = history + [[message, \"\"]]\n",
        "    stop = StopOnTokens()\n",
        "\n",
        "    # useful to debug\n",
        "    # msg = \"history\"\n",
        "    # print(msg)\n",
        "    # print(*history_transformer_format, sep=\"\\n\")\n",
        "    # print(\"***\")\n",
        "\n",
        "    # at each step, we feed the entire history in string format,\n",
        "    # restoring the format used in their dataset with new lines\n",
        "    # and <human>: or <bot>: added before the messages\n",
        "    messages = \"\".join(\n",
        "        [\"\".join(\n",
        "            [\"\\n<human>:\"+item[0], \"\\n<bot>:\"+item[1]]\n",
        "         )\n",
        "        for item in history_transformer_format]\n",
        "    )\n",
        "    # to see what we feed to our net:\n",
        "    # msg = \"string prompt\"\n",
        "    # print(msg)\n",
        "    # print(\"-\" * len(msg))\n",
        "    # print(messages)\n",
        "    # print(\"-\" * 40)\n",
        "\n",
        "    # convert the string into tensors & move to GPU\n",
        "    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer,\n",
        "        # timeout=30.,    # without the timeout, if there's an issue the bot will hang indefinitely\n",
        "        skip_prompt=True, # (haven't implemented the error handling yet ðŸ™ˆ)\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=1000,\n",
        "        temperature=1.0,\n",
        "        pad_token_id=tokenizer.eos_token_id, # mute annoying warning: https://stackoverflow.com/a/71397707\n",
        "        num_beams=1,  # this is for beam search (disabled), see: https://huggingface.co/blog/how-to-generate#beam-search\n",
        "        stopping_criteria=StoppingCriteriaList([stop])\n",
        "    )\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    partial_message  = \"\"\n",
        "    for new_token in streamer:\n",
        "        # seen the format <human>: and \\n<bot> above (when 'messages' is defined)?\n",
        "        # we stream the message *until* we encounter '<', which is by the end\n",
        "        if new_token != '<':\n",
        "            partial_message += new_token\n",
        "            yield partial_message\n",
        "\n",
        "\n",
        "gr.ChatInterface(predict).queue().launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMXQCQzZJImR"
      },
      "source": [
        "How do we know what the stop words are? (This is in part a design choice!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hya1CiBNFbwQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model stop words are:\n",
            "  - `<`\n",
            "  - `<|endoftext|>`\n",
            "If you wanted to know what token was associated with `<`, you'd do the opposite:\n",
            "`<` encoded as: [29]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "print(\"The model stop words are:\")\n",
        "for tok in [29, 0]:\n",
        "    print(f\"  - `{tokenizer.decode([tok])}`\")\n",
        "\n",
        "print(\"If you wanted to know what token was associated with `<`, you'd do the opposite:\")\n",
        "print(\"`<` encoded as:\", tokenizer.encode(\"<\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZU6FJn1jF4g"
      },
      "source": [
        "---\n",
        "\n",
        "## Experiments\n",
        "\n",
        "- You could try to modify this code to work with the latest Llama models by Meta (you must register on [their site](https://ai.meta.com/llama/), then on Huggingface once you get permission, to be able to download the code). After that (same as with various restricted models/datasets/etc. on the Hub), you would need to log into HF:\n",
        "```python\n",
        "from pathlib import Path\n",
        "from huggingface_hub import notebook_login\n",
        "if not (Path.home()/'.huggingface'/'token').exists():\n",
        "    notebook_login()\n",
        "```\n",
        "- Another example that would allow you to play with the cutting-edge LLMs is the [OpenAI example](https://www.gradio.app/guides/creating-a-chatbot-fast#a-streaming-example-using-openai) in the Gradio tutorial. You would first need to register (with credit card) and get an API key on [their website](https://platform.openai.com/)...\n",
        "\n",
        "- Gradio ships with a [`Flagging`](https://www.gradio.app/guides/key-features#styling) logic, that allows you to harvest data from your users for free! You can also implement [`likes`](https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#liking-disliking-chat-messages), that could be interesting!\n",
        "\n",
        "- The current trend these days is to work with multimodality (systems that are able to handle more than one type of data: text and images, for instance, or text and music). See [this last part](https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#adding-markdown-images-audio-or-videos) of the Gradio Chatbot tutorial for examples, as well as the two apps they recommend [project-baize/Baize-7B](https://huggingface.co/spaces/project-baize/chat-with-baize) and [MAGAer13/mPLUG-Owl](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl) (and as said you could clone these projects, study the code, and transform them into your own project)!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
