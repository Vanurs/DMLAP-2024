{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN trained on the MNIST dataset for image classification \n",
    "\n",
    "In this notebook we are going to get familiar with PyTorch, a deep learning library, to train a Convolutional Neural Network (CNN). The network will be trained on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) which contains small images of handwritten numerical digits. By the end of this training, the model should be able to accurately classify images with numerical digits.\n",
    "\n",
    "Training a network on the MNIST dataset has become the 'hello world' of machine learning. \n",
    "\n",
    "First let's do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define our hyperparameters\n",
    "\n",
    "Now let's define our hyperparameters. Add a comment for each variable. What are these variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "momentum = 0.9\n",
    "num_epochs = 10\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learn_rate = 0.001\n",
    "data_path = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image transformations\n",
    "\n",
    "torchvision.transforms.ToTensor converts a python image (PIL) or numpy.ndarray to tensor.\n",
    "\n",
    "torchvision.transforms.Normalize(mean, std, inplace=False) normalizes a tensor image with mean and standard deviation.\n",
    "\n",
    "Look in [here](https://pytorch.org/vision/0.9/transforms.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create our train and test datasets\n",
    "\n",
    "Torchvision provides many built-in datasets in the torchvision.datasets module. MNIST is one of them. You can find more [here](https://pytorch.org/vision/stable/datasets.html).\n",
    "\n",
    "MNIST is composed of a train set (60,000 images) and a test set (10,000 images). Only the train set will be used for the training. The test set is meant to be used after the training is completed, to see how the model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root=data_path, train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root=data_path, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualise a sample image along with its respective label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 1000 # change this number to have a glimpse into another item of the trainset\n",
    "\n",
    "# using matplotlib\n",
    "# plotting for one image\n",
    "plt.figure()\n",
    "plt.title(f\"Label: {mnist_trainset.targets[img_num]}\")\n",
    "plt.imshow(mnist_trainset.data[img_num], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create our train and test dataloaders\n",
    "\n",
    "A DataLoader wraps an iterable around the Dataset, allowing us to iterate over batches of data when training. The DataLoader can shuffle the data at the beginning of each epoch if the shuffle parameter is set to True. Shuffling helps in randomizing the order of data samples and prevents the model from learning any order-specific patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        #\n",
    "        super(ClassificationNetwork, self).__init__()\n",
    "        #\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        #\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        #\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 64)\n",
    "        #\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #\n",
    "        x = self.pool(x)\n",
    "        #\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #\n",
    "        x = self.pool(x)\n",
    "        #\n",
    "        x = torch.flatten(x, 1)\n",
    "        #\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #\n",
    "        x = self.fc2(x)\n",
    "        #\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup core training objects\n",
    "\n",
    "Look into available loss functions [here](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "Look into available optimizers [here](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNetwork()\n",
    "model.to(device)\n",
    "\n",
    "# selecting cross entropy as the loss function for our classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# selecting stochastic gradient descent as our optimization algorithm\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, momentum=momentum)\n",
    "\n",
    "# print model architecture\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # training loop\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # get data\n",
    "        inputs = data.to(device)\n",
    "        labels = target.to(device)\n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        predictions = model(inputs)\n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # update the parameters, i.e. weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # save statistics to plot later\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # normalise cumulative losses to dataset size\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    # added cumulative losses to list to plot later\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot loss values as stored during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Train loss\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cumulative loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test model on the MNIST test dataset\n",
    "\n",
    "A process also known as Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# disabling gradient computation since there is no need to update any parameters\n",
    "with torch.no_grad():\n",
    "    # iterate over batches in the test_loader\n",
    "    for data, target in test_loader:\n",
    "        # forward pass\n",
    "        predictions = model(data)\n",
    "        # get the index of the maximum predicted value (predicted class)\n",
    "        # the underscore is a variable to be ignored - a placeholder for the maximum values\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        # update the total number of samples by adding the current batch\n",
    "        total += target.size(0)\n",
    "        # update the count of correct predictions by summing the correct predictions in the current batch\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy}\\n Number of Correct Predictions: {correct} out of {total}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualise some predictions based on the MNIST test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabling gradient computation since there is no need to update any parameters\n",
    "with torch.no_grad():\n",
    "    num_images = 5\n",
    "    figure = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for i in range(1, num_images + 1):\n",
    "        sample_idx = torch.randint(len(mnist_testset), size=(1,)).item()\n",
    "        img, label = mnist_testset[sample_idx]\n",
    "        img = img.unsqueeze(0).to(device)  # needs an extra batch dimension\n",
    "        output = model(img)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        figure.add_subplot(1, num_images, i)\n",
    "        plt.title(f\"Predicted: {predicted.item()}\\nActual: {label}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualise some predictions based on external inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = './data/sample-numbers/seven-rgb.jpg'\n",
    "\n",
    "# function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    # convert the image to black and white and transform it to match the required input format for the model\n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    # img is an array for the plot and img_tensor is a tensor to pass through the model\n",
    "    return img, img_tensor\n",
    "\n",
    "# preprocess the image\n",
    "image, tensor_image = preprocess_image(img_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # forward pass\n",
    "    prediction = model(tensor_image)\n",
    "    # get the predicted class\n",
    "    _, predicted_class = torch.max(prediction, 1)\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f'Predicted Class: {predicted_class.item()}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/mnist_model_CNN.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks to do in-class and further explore at home\n",
    "\n",
    "**Task 1:** Run all the cells in this code to train a CNN on the MNIST dataset.\n",
    "\n",
    "**Task 2:** This notebook is almost identical to the notebook `01`. The only difference lies in the architecture of the neural network. Go through the respective call where we define the CNN architecture and add comments wherever you see a `#`. Consult [the pytorch documentation page](https://pytorch.org/docs/stable/index.html) for anything you are unsure about. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
