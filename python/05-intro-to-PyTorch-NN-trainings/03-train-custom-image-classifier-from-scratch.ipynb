{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN trained on a custom dataset for image classification\n",
    "\n",
    "In this notebook we are going to train an image classifier from scratch in PyTorch.\n",
    "\n",
    "**Before you proceed with this notebook**, you need to collect the data for the classes you wish to create. To do so, you can follow any of these approaches:\n",
    "\n",
    "- Download public image datasets from Kaggle, e.g. [animal image dataset](https://www.kaggle.com/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals/data).\n",
    "\n",
    "- Install [`gallery-dl`](https://github.com/mikf/gallery-dl). This library allows us to download entire image galleries from sites such as [pinterest](https://www.pinterest.co.uk/), [tumblr](https://www.tumblr.com/), and [bbc](https://www.bbc.co.uk/). Go to [pinterest](https://www.pinterest.co.uk/) and search for a category of your interest. Then, click the filter on the left side and select the option for **Boards**. Select a board and then copy the URL of the pinterest board (e.g. https://www.pinterest.co.uk/user/board-id/) as an argument for the command below. e.g.:\n",
    "\n",
    "    `gallery-dl https://www.pinterest.co.uk/user-id/board-id/`\n",
    "\n",
    "    You can run the above command directly in the terminal or through this notebook by adding a `!` before the command.\n",
    "\n",
    "    Make sure you keep track of the directory where you are when downloading the images.\n",
    "\n",
    "- Record or source videos of objects, and use `ffmpeg` to extract frames from them. This is a quick way to get many variations of an object. Some quick instructions for `ffmpeg`:\n",
    "\n",
    "    Download it from [here](https://ffmpeg.org/download.html)\n",
    "    Then, open the terminal, move into the directory (cd PATH) where you have the video you want to transform into frames, create a folder to save your frames in it and run the command:\n",
    "\n",
    "    `ffmpeg -i file_name.mov -r 1 -s WxH folder_name/%03d.jpeg`\n",
    "\n",
    "    This will extract one video frame per second from the video and will output them in files named 001.jpeg, 002.jpeg, etc. Images will be rescaled to fit the new WxH values (you can skipp -s WxH if you do not wish to resize your images). Look [here](https://ffmpeg.org/ffmpeg.html) for more information on `ffmpeg`.\n",
    "\n",
    "**After you have collected your images** (a folder of images per class), put them into the `data` folder in the current directory, so that you can access them from the path:\n",
    "\n",
    "`./data/custom-image-dataset/`\n",
    "\n",
    "Now you can proceed with the notebook. The following code has been adapted from notebook developed by [Terence Broad](https://github.com/terrybroad).\n",
    "\n",
    "Once you run this code, you can compare it with the code in the notebook `04`. \n",
    "\n",
    "First let's do some imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PILLOW_VERSION' from 'PIL' (c:\\Users\\admin\\miniconda3\\envs\\DMLAP\\lib\\site-packages\\PIL\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DMLAP\\lib\\site-packages\\torchvision\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DMLAP\\lib\\site-packages\\torchvision\\datasets\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvhn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVHN\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mphototour\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PhotoTour\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfakedata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeData\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msemeion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SEMEION\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01momniglot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Omniglot\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DMLAP\\lib\\site-packages\\torchvision\\datasets\\fakedata.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFakeData\u001b[39;00m(data\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A fake dataset that returns randomly generated images and returns them as PIL images\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DMLAP\\lib\\site-packages\\torchvision\\transforms\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DMLAP\\lib\\site-packages\\torchvision\\transforms\\transforms.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     20\u001b[0m     Sequence \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mSequence\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\DMLAP\\lib\\site-packages\\torchvision\\transforms\\functional.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps, ImageEnhance, PILLOW_VERSION\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maccimage\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PILLOW_VERSION' from 'PIL' (c:\\Users\\admin\\miniconda3\\envs\\DMLAP\\lib\\site-packages\\PIL\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define our hyperparameters\n",
    "\n",
    "Now let's define our hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "val_size = 0.3\n",
    "batch_size = 100\n",
    "momentum = 0.9\n",
    "learn_rate = 0.001\n",
    "num_epochs = 10\n",
    "num_classes = 3 # change if you are working with a different number of classes\n",
    "data_path = './data/custom-image-dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image transformations for the train dataset\n",
    "\n",
    "Could you augment your training data by adding more transformations to them?\n",
    "\n",
    "You could randomly change their brightness, contrast, saturation, and hue.\n",
    "\n",
    "You could flip them horizontally or vertically with a 0.5 probability.\n",
    "\n",
    "You could randomly rotate them.\n",
    "\n",
    "Look in [here](https://pytorch.org/vision/stable/transforms.html) and [here](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py) for references and examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(size=(64, 64), antialias=True),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image transformations for the validation dataset\n",
    "\n",
    "Do you need to also add the above transformations to your validation set? Or are the existing ones enough? You need to consider what the purpose of each dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose(\n",
    "    [   \n",
    "        torchvision.transforms.Resize(64, antialias=True),\n",
    "        torchvision.transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create our train and test datasets\n",
    "\n",
    "Here we create our train and validation datasets by splitting the full input dataset into two subsets, with a ratio that was defined with the val_size at the beginning of the notebook. A 70-30 split is quite common.\n",
    "\n",
    "By setting a `random_state`, we are performing the split randomly but in a deterministic way, i.e. we will always get the same random train_test_split as long as we use the same random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation datasets with seperate transforms\n",
    "train_dataset = ImageFolder(data_path, transform=train_transform)\n",
    "val_dataset = ImageFolder(data_path, transform=val_transform)\n",
    "\n",
    "# get length of the full dataset before split, and save it in idx\n",
    "num_train = len(train_dataset)\n",
    "\n",
    "# create an array of idx numbers for each element of the full dataset\n",
    "idx = list(range(num_train))\n",
    "#print(num_train, idx)\n",
    "\n",
    "# perform train / val split for data points\n",
    "train_indices, val_indices = train_test_split(idx, test_size=val_size, random_state=42)\n",
    "\n",
    "# override datasets to only be samples for each split\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "val_dataset = Subset(val_dataset, val_indices)\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot sample images from the two datasets\n",
    "\n",
    "In the next two cells we are plotting a sample of images from each dataset. See how the data augmentation transforms affect the images of the training set, compared to the images in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training images\n",
    "sample_batch = next(iter(train_loader))[0][:64].to(device)\n",
    "\n",
    "# create a grid of images\n",
    "img_grid = vutils.make_grid(sample_batch, padding=2, normalize=True)\n",
    "\n",
    "# convert to NumPy and transpose dimensions for matplotlib\n",
    "img_grid_np = np.transpose(img_grid.cpu().numpy(), (1, 2, 0))\n",
    "\n",
    "# plot the grid of images\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(img_grid_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training images\n",
    "sample_batch = next(iter(val_loader))[0][:64].to(device)\n",
    "\n",
    "# create a grid of images\n",
    "img_grid = vutils.make_grid(sample_batch, padding=2, normalize=True)\n",
    "\n",
    "# convert to NumPy and transpose dimensions for matplotlib\n",
    "img_grid_np = np.transpose(img_grid.cpu().numpy(), (1, 2, 0))\n",
    "\n",
    "# plot the grid of images\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Validation Images\")\n",
    "plt.imshow(img_grid_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv3 = nn.Conv2d(128, 128, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(2048, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup core training objects\n",
    "\n",
    "Look into available loss functions [here](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "Look into available optimizers [here](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNetwork()\n",
    "model.to(device)\n",
    "\n",
    "# selecting cross entropy as the loss function for our classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# selecting stochastic gradient descent as our optimization algorithm\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, momentum=momentum)\n",
    "\n",
    "# print model architecture\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training\n",
    "\n",
    "What is different here from the training we did for the MNIST CNN classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # training loop\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # get data\n",
    "        inputs = data.to(device)\n",
    "        labels = target.to(device)\n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        predictions = model(inputs)\n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # update the parameters, i.e. weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # save statistics to plot later\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # validation loop\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # get data\n",
    "            inputs = data.to(device)\n",
    "            labels = target.to(device)\n",
    "            # forward pass, no backpropagation and optimisation\n",
    "            predictions = model(inputs)\n",
    "            # compute the loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            # save statistics to plot later\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # normalise cumulative losses to dataset size\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # added cumulative losses to lists to plot later\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}, val loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot training vs validation loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Train vs validation loss\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.plot(val_losses,label=\"val\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cumulative loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/img_classfier_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks to do in-class and further explore at home\n",
    "\n",
    "**Task 1:** Create your own dataset with 2, 3, or more classes, based on one of the suggested approaches. It would be effective to have at least 1000 images/class. Create a folder for each one of your classes and save the respective images there. Then move all of the class folders in `./data/custom-image-dataset`. Make sure you manually clean-up your data before training, to remove any irrelevant or destroyed images.\n",
    "\n",
    "**Task 2:** Run all the cells in this code to train a classifier on your custom dataset.\n",
    "\n",
    "**Task 3:** Add image transformations on the training dataset. Look in [here](https://pytorch.org/vision/stable/transforms.html) and [here](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py) for references and examples. \n",
    "\n",
    "**Task 4:** Create a new notebook where you call the model that you just saved from this training and test it on some new unseen data. Look into the MNIST notebooks 01 and 02, as well as the PyTorch website, to figure out how you can call and use an already trained model.\n",
    "\n",
    "**Bonus Challenges:**\n",
    "\n",
    "**Bonus 1:** Look into the concept of Early Stopping. What is it? Could it be useful for our training? How? Attempt to implement it by adding the following lines of code after the training loop is completed:\n",
    "\n",
    "   `if val_loss < best_loss:`\n",
    "        \n",
    "        `best_loss = val_loss`\n",
    "        \n",
    "        `torch.save(model.state_dict(), 'best_model_from_scratch.pt')`\n",
    "\n",
    "For this to work, you will have to initialise best_loss with a high value before you enter the training loop.\n",
    "\n",
    "**Bonus 2:** In this example you are building your classifier from scratch, i.e. you decide yourself what the architecture of the network is and you train it from the very beginning. Could you explore a way for training your classifier based on a pre-trained model? There are many available pre-trained models in [the torchvision models library](https://pytorch.org/vision/stable/models.html), like [ResNet](https://arxiv.org/abs/1512.03385) which is trained on [imagenet dataset](https://www.image-net.org/). This approach will require a few changes and additions in your notebook. Attempt it if you are feeling adventurous!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
